# Measuring the divergence of two probability distributions

- Kullback–Leibler Divergence
    - non-symmetric
    - also called relative entropy and I-divergence
- Jensen–Shannon Divergence
    - symmetric version of KL Divergence
    - It is also known as Information radius or total divergence to the average.
- Kolmogorov–Smirnov Test
    - Used for continuous non-parametric one-dimension data distribution



### References

- [https://en.wikipedia.org/wiki/Statistical_distance](https://en.wikipedia.org/wiki/Statistical_distance)