# Word Embedding

## Traditional Word Embedding

Each word token has one vector representation regardless of the context.

### Word2Vec

[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

- Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean

[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)

- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean

### GloVe 

[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (EMNLP 2014)

- Jeffrey Pennington, Richard Socher, Christopher D. Manning

## Contextualized Word Embedding

### ELMO 

[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) (NAACL 2018)

- Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer

### BERT 

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (NAACL 2019)

- Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

### GPT

GPT-2: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019)

GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (2020)